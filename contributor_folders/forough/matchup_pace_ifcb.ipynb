{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195f178-f1df-460e-816b-416eb2b091ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "IFCB ↔ PACE OCI L3M Rrs Matchup Deliverable Script\n",
    "--------------------------------------------------\n",
    "Creates a clean, reproducible deliverable for teammates:\n",
    "- PACE L3M Rrs matchups (nearest pixel) for dates/locations in IFCB CSV\n",
    "- Spectra plots per date/location\n",
    "- Montage maps (clean land mask + visible black point)\n",
    "- Feature tables (wide + long)\n",
    "- IFCB daily labels aggregated into coarse phytoplankton groups\n",
    "- Merged dataset (features + labels) suitable for ML training\n",
    "- QC summary + README + requirements\n",
    "\n",
    "INPUT\n",
    "-----\n",
    "IFCB CSV must include at least:\n",
    "- date (YYYY-MM-DD or YYYYMMDD)\n",
    "- latitude\n",
    "- longitude\n",
    "\n",
    "It should also include SOME taxonomic/class label column for grouping.\n",
    "This script auto-detects from common names, or you can set IFCB_LABEL_COL env var.\n",
    "\n",
    "PACE\n",
    "----\n",
    "Uses earthaccess to search and download OB.DAAC product:\n",
    "short_name = PACE_OCI_L3M_RRS  (version varies server-side)\n",
    "\n",
    "USAGE\n",
    "-----\n",
    "pip install -r requirements.txt\n",
    "earthaccess login\n",
    "python ifcb_pace_l3m_matchup_deliverable.py\n",
    "\n",
    "Optional env vars:\n",
    "- IFCB_CSV: path to ifcb.csv\n",
    "- OUT_DIR: outputs directory\n",
    "- PACE_SHORTNAME: default PACE_OCI_L3M_RRS\n",
    "- TARGET_NM: montage wavelength (default 442)\n",
    "- MONTAGE_HALF_WINDOW_DEG: default 1.0\n",
    "- MONTAGE_NCOLS: default 4\n",
    "- IFCB_LABEL_COL: force label column name\n",
    "- IFCB_SITE_COL: optional site/station column name to include in grouping/merge\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import pathlib\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def ensure_dir(p: str | pathlib.Path) -> pathlib.Path:\n",
    "    p = pathlib.Path(p)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def safe_fname(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", s)\n",
    "\n",
    "def parse_date_series(date_ser: pd.Series) -> pd.Series:\n",
    "    s = date_ser.astype(str).str.strip()\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "    m = dt.isna()\n",
    "    if m.any():\n",
    "        dt2 = pd.to_datetime(s[m], errors=\"coerce\", format=\"%Y%m%d\")\n",
    "        dt.loc[m] = dt2\n",
    "    if dt.isna().any():\n",
    "        bad = s[dt.isna()].unique()[:10]\n",
    "        raise ValueError(f\"Could not parse some dates. Examples: {bad}\")\n",
    "    return dt.dt.normalize()\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2) -> float:\n",
    "    R = 6371.0088\n",
    "    phi1 = np.deg2rad(lat1)\n",
    "    phi2 = np.deg2rad(lat2)\n",
    "    dphi = np.deg2rad(lat2 - lat1)\n",
    "    dl = np.deg2rad(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dl/2)**2\n",
    "    return float(2 * R * np.arcsin(np.sqrt(a)))\n",
    "\n",
    "def pick_first_existing_col(df: pd.DataFrame, candidates: list[str]) -> str | None:\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in lower_map:\n",
    "            return lower_map[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def write_text(fp: pathlib.Path, text: str):\n",
    "    fp.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# IFCB label grouping\n",
    "# -----------------------------\n",
    "\n",
    "def default_group_mapping() -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    EDIT HERE if you want custom mapping.\n",
    "    Keys are regex patterns matched against the IFCB label text (lowercased).\n",
    "    Values are the coarse groups for the ML target.\n",
    "\n",
    "    You can expand patterns as you learn your IFCB taxonomy strings.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        r\"\\bdiatom\": \"diatoms\",\n",
    "        r\"\\bbacillari\": \"diatoms\",\n",
    "        r\"\\b(dino|dinoflag)\": \"dinoflagellates\",\n",
    "        r\"\\bhapto\": \"haptophytes\",\n",
    "        r\"\\bemiliania\": \"haptophytes\",\n",
    "        r\"\\bsynech|prochl|cyan\": \"cyanobacteria_like\",\n",
    "        r\"\\bcryptophy\": \"cryptophytes\",\n",
    "        # fallback handled separately\n",
    "    }\n",
    "\n",
    "def map_label_to_group(label: str, mapping: dict[str, str]) -> str:\n",
    "    if label is None or (isinstance(label, float) and np.isnan(label)):\n",
    "        return \"other_unknown\"\n",
    "    s = str(label).strip().lower()\n",
    "    if s == \"\" or s in {\"nan\", \"none\"}:\n",
    "        return \"other_unknown\"\n",
    "    for pat, grp in mapping.items():\n",
    "        if re.search(pat, s):\n",
    "            return grp\n",
    "    return \"other_unknown\"\n",
    "\n",
    "def aggregate_ifcb_labels(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    lat_col: str,\n",
    "    lon_col: str,\n",
    "    label_col: str,\n",
    "    site_col: str | None,\n",
    "    out_fp: pathlib.Path,\n",
    ") -> pd.DataFrame:\n",
    "    mapping = default_group_mapping()\n",
    "    tmp = df[[date_col, lat_col, lon_col, label_col] + ([site_col] if site_col else [])].copy()\n",
    "\n",
    "    tmp[\"group\"] = tmp[label_col].apply(lambda x: map_label_to_group(x, mapping))\n",
    "\n",
    "    group_keys = [date_col, lat_col, lon_col]\n",
    "    if site_col:\n",
    "        group_keys.append(site_col)\n",
    "\n",
    "    counts = tmp.groupby(group_keys + [\"group\"]).size().rename(\"n\").reset_index()\n",
    "\n",
    "    # total per day/location\n",
    "    totals = counts.groupby(group_keys)[\"n\"].sum().rename(\"n_total\").reset_index()\n",
    "\n",
    "    out = counts.merge(totals, on=group_keys, how=\"left\")\n",
    "    out[\"fraction\"] = out[\"n\"] / out[\"n_total\"]\n",
    "\n",
    "    # pivot wide fractions (nice for ML labels)\n",
    "    frac_wide = out.pivot_table(\n",
    "        index=group_keys,\n",
    "        columns=\"group\",\n",
    "        values=\"fraction\",\n",
    "        fill_value=0.0,\n",
    "        aggfunc=\"mean\",\n",
    "    ).reset_index()\n",
    "\n",
    "    # also pivot counts wide\n",
    "    count_wide = out.pivot_table(\n",
    "        index=group_keys,\n",
    "        columns=\"group\",\n",
    "        values=\"n\",\n",
    "        fill_value=0,\n",
    "        aggfunc=\"sum\",\n",
    "    ).reset_index()\n",
    "    count_wide.columns = [c if isinstance(c, str) else str(c) for c in count_wide.columns]\n",
    "    frac_wide.columns = [c if isinstance(c, str) else str(c) for c in frac_wide.columns]\n",
    "\n",
    "    # rename columns to make explicit\n",
    "    for c in list(count_wide.columns):\n",
    "        if c not in group_keys:\n",
    "            count_wide = count_wide.rename(columns={c: f\"count_{c}\"})\n",
    "    for c in list(frac_wide.columns):\n",
    "        if c not in group_keys:\n",
    "            frac_wide = frac_wide.rename(columns={c: f\"frac_{c}\"})\n",
    "\n",
    "    labels = totals.merge(count_wide, on=group_keys, how=\"left\").merge(frac_wide, on=group_keys, how=\"left\")\n",
    "\n",
    "    labels.to_csv(out_fp, index=False)\n",
    "    return labels\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PACE access + matchup\n",
    "# -----------------------------\n",
    "\n",
    "def search_pace_l3m_rrs_for_date(date: datetime, short_name: str):\n",
    "    t0 = date.strftime(\"%Y-%m-%d\")\n",
    "    t1 = (date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    return earthaccess.search_data(short_name=short_name, temporal=(t0, t1), count=50)\n",
    "\n",
    "def pick_best_granule(granules, prefer_resolution=\"4km\"):\n",
    "    if not granules:\n",
    "        return None\n",
    "    for g in granules:\n",
    "        try:\n",
    "            for u in g.data_links():\n",
    "                if prefer_resolution.lower() in u.lower():\n",
    "                    return g\n",
    "        except Exception:\n",
    "            pass\n",
    "    return granules[0]\n",
    "\n",
    "def open_rrs_dataset(granule) -> tuple[xr.Dataset, str]:\n",
    "    paths = earthaccess.open([granule])\n",
    "    if not paths:\n",
    "        raise RuntimeError(\"earthaccess.open returned no paths.\")\n",
    "    fp = paths[0]\n",
    "    # robust open\n",
    "    for engine in (None, \"h5netcdf\", \"netcdf4\"):\n",
    "        try:\n",
    "            ds = xr.open_dataset(fp, engine=engine) if engine else xr.open_dataset(fp)\n",
    "            return ds, str(fp)\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError(f\"Could not open dataset: {fp}\")\n",
    "\n",
    "def nearest_rrs_spectrum(ds: xr.Dataset, lat: float, lon: float):\n",
    "    lat_name = \"lat\" if \"lat\" in ds.coords else (\"latitude\" if \"latitude\" in ds.coords else None)\n",
    "    lon_name = \"lon\" if \"lon\" in ds.coords else (\"longitude\" if \"longitude\" in ds.coords else None)\n",
    "    if lat_name is None or lon_name is None:\n",
    "        raise KeyError(f\"Could not find lat/lon coords in dataset. Coords: {list(ds.coords)}\")\n",
    "    if \"Rrs\" not in ds.variables:\n",
    "        raise KeyError(f\"Could not find variable 'Rrs'. Variables: {list(ds.variables)}\")\n",
    "    if \"wavelength\" not in ds.coords and \"wavelength\" not in ds.variables:\n",
    "        raise KeyError(\"Could not find 'wavelength' coordinate/variable.\")\n",
    "\n",
    "    # handle 0..360 lon in file\n",
    "    lons = ds[lon_name].values\n",
    "    lon_use = lon + 360.0 if (np.nanmin(lons) >= 0 and lon < 0) else lon\n",
    "\n",
    "    rrs = ds[\"Rrs\"]\n",
    "    wl = ds[\"wavelength\"].values\n",
    "\n",
    "    sel = rrs.sel({lat_name: lat, lon_name: lon_use}, method=\"nearest\")\n",
    "    lat_near = float(sel[lat_name].values)\n",
    "    lon_near = float(sel[lon_name].values)\n",
    "    lon_near_out = lon_near - 360.0 if lon_near > 180 else lon_near\n",
    "    spec = sel.values\n",
    "\n",
    "    return wl, spec, lat_near, lon_near_out\n",
    "\n",
    "def rrs_at_wavelength(ds: xr.Dataset, lat: float, lon: float, target_nm: float):\n",
    "    wl, spec, lat_near, lon_near = nearest_rrs_spectrum(ds, lat, lon)\n",
    "    idx = int(np.nanargmin(np.abs(wl - target_nm)))\n",
    "    return float(wl[idx]), float(spec[idx]), lat_near, lon_near\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Plotting\n",
    "# -----------------------------\n",
    "\n",
    "def plot_spectrum(wl, spec, title: str, out_png: pathlib.Path):\n",
    "    plt.figure(figsize=(8.5, 4.8))\n",
    "    plt.plot(wl, spec)\n",
    "    plt.xlabel(\"Wavelength (nm)\")\n",
    "    plt.ylabel(\"Rrs (sr$^{-1}$)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "def plot_montage_maps_clean(\n",
    "    ds_list,\n",
    "    meta_list,\n",
    "    *,\n",
    "    target_nm: float,\n",
    "    out_png: pathlib.Path,\n",
    "    half_window_deg: float = 1.0,\n",
    "    ncols: int = 4,\n",
    "):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    from matplotlib import cm\n",
    "\n",
    "    def _subset_rrs(ds, lat0, lon0, wli, half_window_deg):\n",
    "        lon_name = \"lon\" if \"lon\" in ds.coords else \"longitude\"\n",
    "        lat_name = \"lat\" if \"lat\" in ds.coords else \"latitude\"\n",
    "\n",
    "        # selection lon handling (0..360)\n",
    "        lons = ds[lon_name].values\n",
    "        lon_use = lon0 + 360.0 if (np.nanmin(lons) >= 0 and lon0 < 0) else lon0\n",
    "\n",
    "        lat_min, lat_max = lat0 - half_window_deg, lat0 + half_window_deg\n",
    "        lon_min, lon_max = lon_use - half_window_deg, lon_use + half_window_deg\n",
    "\n",
    "        rrs2d = ds[\"Rrs\"].isel(wavelength=wli)\n",
    "\n",
    "        latv = ds[lat_name]\n",
    "        lonv = ds[lon_name]\n",
    "        mask = (latv >= lat_min) & (latv <= lat_max) & (lonv >= lon_min) & (lonv <= lon_max)\n",
    "        sub = rrs2d.where(mask, drop=True)\n",
    "\n",
    "        # if empty\n",
    "        if (sub.sizes.get(lat_name, 0) < 2) or (sub.sizes.get(lon_name, 0) < 2):\n",
    "            return None, None, None, None, lon_name, lat_name, lon_use\n",
    "\n",
    "        # ONLY NOW load small window into memory\n",
    "        sub_vals = sub.values.astype(float)\n",
    "\n",
    "        fill = sub.attrs.get(\"_FillValue\", None)\n",
    "        if fill is not None:\n",
    "            sub_vals = np.where(sub_vals == float(fill), np.nan, sub_vals)\n",
    "\n",
    "        # mask invalid/land-ish\n",
    "        sub_vals = np.where(sub_vals <= 0, np.nan, sub_vals)\n",
    "\n",
    "        lon_plot = sub[lon_name].values.astype(float)\n",
    "        lat_plot = sub[lat_name].values.astype(float)\n",
    "\n",
    "        # wrap lon to [-180,180) for plotting\n",
    "        if np.nanmax(lon_plot) > 180:\n",
    "            lon_plot = ((lon_plot + 180) % 360) - 180\n",
    "            order = np.argsort(lon_plot)\n",
    "            lon_plot = lon_plot[order]\n",
    "            sub_vals = sub_vals[:, order]\n",
    "\n",
    "        return lon_plot, lat_plot, sub_vals, sub, lon_name, lat_name, lon_use\n",
    "\n",
    "    n = len(ds_list)\n",
    "    if n == 0:\n",
    "        return\n",
    "\n",
    "    # wavelength indices (cheap)\n",
    "    wlis = []\n",
    "    wl_used_list = []\n",
    "    for ds in ds_list:\n",
    "        wl = ds[\"wavelength\"].values\n",
    "        wli = int(np.nanargmin(np.abs(wl - target_nm)))\n",
    "        wlis.append(wli)\n",
    "        wl_used_list.append(float(wl[wli]))\n",
    "\n",
    "    # --- shared color scale using ONLY small windows ---\n",
    "    window_values = []\n",
    "    for ds, meta, wli in zip(ds_list, meta_list, wlis):\n",
    "        lat0 = float(meta[\"lat\"])\n",
    "        lon0 = float(meta[\"lon\"])\n",
    "        lon_plot, lat_plot, sub_vals, _, *_ = _subset_rrs(ds, lat0, lon0, wli, half_window_deg)\n",
    "        if sub_vals is None:\n",
    "            continue\n",
    "        vv = sub_vals[np.isfinite(sub_vals)]\n",
    "        if vv.size:\n",
    "            window_values.append(vv)\n",
    "\n",
    "    if window_values:\n",
    "        allv = np.concatenate(window_values)\n",
    "        vmin = float(np.nanpercentile(allv, 5))\n",
    "        vmax = float(np.nanpercentile(allv, 98))\n",
    "        if not np.isfinite(vmin) or not np.isfinite(vmax) or vmax <= vmin:\n",
    "            vmin, vmax = float(np.nanmin(allv)), float(np.nanmax(allv))\n",
    "    else:\n",
    "        vmin, vmax = 0.0, 1.0\n",
    "\n",
    "    cmap = cm.get_cmap(\"viridis\").copy()\n",
    "    cmap.set_bad(color=\"white\", alpha=1.0)\n",
    "\n",
    "    ncols = max(1, ncols)\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "    fig = plt.figure(figsize=(4.2 * ncols, 3.6 * nrows))\n",
    "    last_im = None\n",
    "\n",
    "    for i, (ds, meta, wli, wl_used) in enumerate(zip(ds_list, meta_list, wlis, wl_used_list), start=1):\n",
    "        ax = plt.subplot(nrows, ncols, i, projection=ccrs.PlateCarree())\n",
    "\n",
    "        lat0 = float(meta[\"lat\"])\n",
    "        lon0 = float(meta[\"lon\"])\n",
    "\n",
    "        lon_plot, lat_plot, sub_vals, _, *_ = _subset_rrs(ds, lat0, lon0, wli, half_window_deg)\n",
    "\n",
    "        if sub_vals is None:\n",
    "            ax.set_title(f\"{meta['date_str']}\\n(empty window)\", fontsize=10)\n",
    "            ax.add_feature(cfeature.LAND, facecolor=\"white\", zorder=5)\n",
    "            ax.add_feature(cfeature.COASTLINE.with_scale(\"50m\"), linewidth=0.7, zorder=6)\n",
    "            ax.plot(lon0, lat0, \"o\", ms=7, color=\"black\", markeredgecolor=\"white\",\n",
    "                    markeredgewidth=1.8, transform=ccrs.PlateCarree(), zorder=10)\n",
    "            continue\n",
    "\n",
    "        last_im = ax.pcolormesh(\n",
    "            lon_plot,\n",
    "            lat_plot,\n",
    "            sub_vals,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            shading=\"nearest\",\n",
    "            cmap=cmap,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            rasterized=True,\n",
    "            zorder=1,\n",
    "        )\n",
    "\n",
    "        # clean coast/land\n",
    "        ax.add_feature(cfeature.LAND, facecolor=\"white\", zorder=5)\n",
    "        ax.add_feature(cfeature.COASTLINE.with_scale(\"50m\"), linewidth=0.7, zorder=6)\n",
    "\n",
    "        # visible point\n",
    "        ax.plot(\n",
    "            lon0, lat0,\n",
    "            marker=\"o\",\n",
    "            markersize=9,\n",
    "            color=\"black\",\n",
    "            markeredgecolor=\"white\",\n",
    "            markeredgewidth=1.6,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "        ax.set_extent([float(np.min(lon_plot)), float(np.max(lon_plot)),\n",
    "                       float(np.min(lat_plot)), float(np.max(lat_plot))],\n",
    "                      crs=ccrs.PlateCarree())\n",
    "\n",
    "        ax.set_title(f\"{meta['date_str']}\\nRrs ~ {wl_used:.0f} nm\", fontsize=10)\n",
    "\n",
    "    if last_im is not None:\n",
    "        cax = fig.add_axes([0.92, 0.15, 0.015, 0.7])\n",
    "        cb = plt.colorbar(last_im, cax=cax)\n",
    "        cb.set_label(\"Rrs (sr$^{-1}$)\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.savefig(out_png, dpi=240)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main pipeline\n",
    "# -----------------------------\n",
    "\n",
    "def main(\n",
    "    ifcb_csv: str,\n",
    "    out_dir: str,\n",
    "    pace_short_name: str,\n",
    "    target_nm: float,\n",
    "    montage_half_window_deg: float,\n",
    "    montage_ncols: int,\n",
    "):\n",
    "    outp = ensure_dir(out_dir)\n",
    "    log_fp = outp / \"log.txt\"\n",
    "\n",
    "    def log(msg: str):\n",
    "        ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        line = f\"[{ts}] {msg}\"\n",
    "        print(line)\n",
    "        with open(log_fp, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "    # Load IFCB\n",
    "    df = pd.read_csv(ifcb_csv)\n",
    "    # auto-detect required columns\n",
    "    date_col = pick_first_existing_col(df, [\"date\", \"Date\", \"sample_date\"])\n",
    "    lat_col  = pick_first_existing_col(df, [\"latitude\", \"lat\", \"Latitude\", \"LAT\"])\n",
    "    lon_col  = pick_first_existing_col(df, [\"longitude\", \"lon\", \"Longitude\", \"LON\", \"long\"])\n",
    "    if not (date_col and lat_col and lon_col):\n",
    "        raise KeyError(f\"Need date/lat/lon columns. Found: {list(df.columns)}\")\n",
    "\n",
    "    df[date_col] = parse_date_series(df[date_col])\n",
    "    df[lat_col] = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
    "    df[lon_col] = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
    "\n",
    "    # optional site/station column\n",
    "    site_col = os.environ.get(\"IFCB_SITE_COL\", \"\").strip() or None\n",
    "    if site_col and site_col not in df.columns:\n",
    "        log(f\"WARNING: IFCB_SITE_COL={site_col} not found; ignoring.\")\n",
    "        site_col = None\n",
    "    if not site_col:\n",
    "        site_col = pick_first_existing_col(df, [\"site\", \"station\", \"Station\", \"site_id\", \"station_id\"])\n",
    "\n",
    "    # label column detection\n",
    "    forced_label = os.environ.get(\"IFCB_LABEL_COL\", \"\").strip()\n",
    "    if forced_label:\n",
    "        if forced_label not in df.columns:\n",
    "            raise KeyError(f\"IFCB_LABEL_COL={forced_label} not in columns.\")\n",
    "        label_col = forced_label\n",
    "    else:\n",
    "        label_col = pick_first_existing_col(df, [\n",
    "            \"autoclass\", \"class\", \"taxon\", \"taxonomy\", \"label\", \"species\", \"scientific_name\",\n",
    "            \"classification\", \"annotated_class\", \"predicted_class\"\n",
    "        ])\n",
    "    if not label_col:\n",
    "        log(\"WARNING: No label column detected. Labels file will NOT be created. \"\n",
    "            \"Set IFCB_LABEL_COL to your column name if you have it.\")\n",
    "    else:\n",
    "        log(f\"Detected IFCB label column: {label_col}\")\n",
    "\n",
    "    # Unique points (date + lat/lon [+ site])\n",
    "    keys = [date_col, lat_col, lon_col] + ([site_col] if site_col else [])\n",
    "    pts = df[keys].dropna().drop_duplicates().sort_values(date_col)\n",
    "\n",
    "    log(f\"Loaded {len(df):,} IFCB rows, unique date/lat/lon combos: {len(pts)}\")\n",
    "\n",
    "    # Earthdata login\n",
    "    log(\"Logging into Earthdata (earthaccess)...\")\n",
    "    earthaccess.login()\n",
    "\n",
    "    # storage\n",
    "    feature_rows = []\n",
    "    long_rows = []\n",
    "    ds_for_montage = []\n",
    "    meta_for_montage = []\n",
    "    ok_count, fail_count = 0, 0\n",
    "\n",
    "    # iterate unique date/location combos\n",
    "    for _, r in pts.iterrows():\n",
    "        date = pd.Timestamp(r[date_col]).to_pydatetime()\n",
    "        date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        lat = float(r[lat_col])\n",
    "        lon = float(r[lon_col])\n",
    "        site_val = str(r[site_col]) if site_col else None\n",
    "\n",
    "        log(f\"Searching PACE L3M Rrs for {date_str} ...\")\n",
    "        granules = search_pace_l3m_rrs_for_date(date, short_name=pace_short_name)\n",
    "        if not granules:\n",
    "            fail_count += 1\n",
    "            feature_rows.append({\n",
    "                \"date\": date_str, \"latitude\": lat, \"longitude\": lon,\n",
    "                \"site\": site_val,\n",
    "                \"status\": \"no_granule\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        g = pick_best_granule(granules, prefer_resolution=\"4km\")\n",
    "        if g is None:\n",
    "            fail_count += 1\n",
    "            feature_rows.append({\n",
    "                \"date\": date_str, \"latitude\": lat, \"longitude\": lon,\n",
    "                \"site\": site_val,\n",
    "                \"status\": \"no_selection\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ds, local_fp = open_rrs_dataset(g)\n",
    "        except Exception as e:\n",
    "            fail_count += 1\n",
    "            feature_rows.append({\n",
    "                \"date\": date_str, \"latitude\": lat, \"longitude\": lon,\n",
    "                \"site\": site_val,\n",
    "                \"status\": \"open_failed\",\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            wl, spec, lat_near, lon_near = nearest_rrs_spectrum(ds, lat, lon)\n",
    "        except Exception as e:\n",
    "            fail_count += 1\n",
    "            feature_rows.append({\n",
    "                \"date\": date_str, \"latitude\": lat, \"longitude\": lon,\n",
    "                \"site\": site_val,\n",
    "                \"status\": \"extract_failed\",\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "            try:\n",
    "                ds.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            continue\n",
    "\n",
    "        ok_count += 1\n",
    "\n",
    "        # distance to nearest pixel\n",
    "        dist_km = haversine_km(lat, lon, lat_near, lon_near)\n",
    "\n",
    "        # Save spectra plot\n",
    "        spec_png = outp / safe_fname(f\"spectra_{date_str}_lat{lat:.4f}_lon{lon:.4f}.png\")\n",
    "        plot_spectrum(\n",
    "            wl,\n",
    "            spec,\n",
    "            title=f\"PACE OCI L3M Rrs spectrum\\n{date_str} @ ({lat:.4f}, {lon:.4f}) \"\n",
    "                  f\"nearest=({lat_near:.4f}, {lon_near:.4f}) dist={dist_km:.2f} km\",\n",
    "            out_png=spec_png,\n",
    "        )\n",
    "        log(f\"Saved spectra: {spec_png.name}\")\n",
    "\n",
    "        # Save montage inputs\n",
    "        ds_for_montage.append(ds)\n",
    "        meta_for_montage.append({\n",
    "            \"date_str\": date_str,\n",
    "            \"lat\": lat,\n",
    "            \"lon\": lon,\n",
    "        })\n",
    "\n",
    "        # row wide features\n",
    "        row = {\n",
    "            \"date\": date_str,\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"nearest_lat\": lat_near,\n",
    "            \"nearest_lon\": lon_near,\n",
    "            \"distance_km\": dist_km,\n",
    "            \"pace_short_name\": pace_short_name,\n",
    "            \"status\": \"ok\",\n",
    "            \"pace_local_file\": local_fp,\n",
    "        }\n",
    "        if site_col:\n",
    "            row[\"site\"] = site_val\n",
    "\n",
    "        # target wavelength\n",
    "        w_used, r_target, _, _ = rrs_at_wavelength(ds, lat, lon, target_nm)\n",
    "        row[\"target_nm_requested\"] = float(target_nm)\n",
    "        row[\"target_nm_used\"] = float(w_used)\n",
    "        row[\"rrs_target\"] = float(r_target) if np.isfinite(r_target) else np.nan\n",
    "\n",
    "        # add all wavelengths as rrs_<nm>\n",
    "        for w, v in zip(wl, spec):\n",
    "            w_int = int(round(float(w)))\n",
    "            row[f\"rrs_{w_int}\"] = float(v) if np.isfinite(v) else np.nan\n",
    "\n",
    "            long_rows.append({\n",
    "                \"date\": date_str,\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lon,\n",
    "                \"site\": site_val,\n",
    "                \"wavelength_nm\": float(w),\n",
    "                \"rrs\": float(v) if np.isfinite(v) else np.nan,\n",
    "                \"nearest_lat\": lat_near,\n",
    "                \"nearest_lon\": lon_near,\n",
    "                \"distance_km\": dist_km,\n",
    "                \"status\": \"ok\",\n",
    "            })\n",
    "\n",
    "        feature_rows.append(row)\n",
    "\n",
    "    # Write feature tables\n",
    "    features_wide = pd.DataFrame(feature_rows)\n",
    "    out_wide = outp / \"matchups_features_wide.csv\"\n",
    "    features_wide.to_csv(out_wide, index=False)\n",
    "    log(f\"Wrote: {out_wide}\")\n",
    "\n",
    "    features_long = pd.DataFrame(long_rows)\n",
    "    out_long = outp / \"matchups_features_long.csv\"\n",
    "    features_long.to_csv(out_long, index=False)\n",
    "    log(f\"Wrote: {out_long}\")\n",
    "\n",
    "    # Montage map (clean)\n",
    "    if ds_for_montage:\n",
    "        montage_png = outp / safe_fname(f\"montage_rrs_{int(round(target_nm))}nm.png\")\n",
    "        plot_montage_maps_clean(\n",
    "            ds_for_montage,\n",
    "            meta_for_montage,\n",
    "            target_nm=target_nm,\n",
    "            out_png=montage_png,\n",
    "            half_window_deg=montage_half_window_deg,\n",
    "            ncols=montage_ncols,\n",
    "        )\n",
    "        log(f\"Wrote: {montage_png}\")\n",
    "\n",
    "    # Close datasets\n",
    "    for ds in ds_for_montage:\n",
    "        try:\n",
    "            ds.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Labels aggregation (if possible)\n",
    "    labels_fp = outp / \"ifcb_labels_daily.csv\"\n",
    "    labels = None\n",
    "    if label_col:\n",
    "        labels = aggregate_ifcb_labels(\n",
    "            df=df,\n",
    "            date_col=date_col,\n",
    "            lat_col=lat_col,\n",
    "            lon_col=lon_col,\n",
    "            label_col=label_col,\n",
    "            site_col=site_col,\n",
    "            out_fp=labels_fp,\n",
    "        )\n",
    "        log(f\"Wrote: {labels_fp}\")\n",
    "\n",
    "    # Merge features + labels (deliverable for ML)\n",
    "    merged_fp = outp / \"matchups_with_labels.csv\"\n",
    "    if labels is not None:\n",
    "        # align column names for merge\n",
    "        f = features_wide.copy()\n",
    "        f[\"date\"] = pd.to_datetime(f[\"date\"])\n",
    "        lbl = labels.copy()\n",
    "        lbl = lbl.rename(columns={\n",
    "            date_col: \"date\",\n",
    "            lat_col: \"latitude\",\n",
    "            lon_col: \"longitude\",\n",
    "        })\n",
    "        lbl[\"date\"] = pd.to_datetime(lbl[\"date\"])\n",
    "\n",
    "        merge_keys = [\"date\", \"latitude\", \"longitude\"]\n",
    "        if site_col and \"site\" in f.columns and \"site\" in lbl.columns:\n",
    "            merge_keys.append(\"site\")\n",
    "\n",
    "        merged = f.merge(lbl, on=merge_keys, how=\"left\")\n",
    "        merged[\"date\"] = merged[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        merged.to_csv(merged_fp, index=False)\n",
    "        log(f\"Wrote: {merged_fp}\")\n",
    "    else:\n",
    "        # still write a copy for consistency\n",
    "        features_wide.to_csv(merged_fp, index=False)\n",
    "        log(f\"Wrote (no labels available): {merged_fp}\")\n",
    "\n",
    "    # QC summary\n",
    "    qc = {\n",
    "        \"n_ifcb_rows\": int(len(df)),\n",
    "        \"n_unique_date_lat_lon\": int(len(pts)),\n",
    "        \"n_matchup_ok\": int(ok_count),\n",
    "        \"n_matchup_failed\": int(fail_count),\n",
    "        \"pace_product_short_name\": pace_short_name,\n",
    "        \"target_nm_for_montage\": float(target_nm),\n",
    "        \"montage_half_window_deg\": float(montage_half_window_deg),\n",
    "    }\n",
    "    qc_df = pd.DataFrame([qc])\n",
    "    qc_fp = outp / \"qc_summary.csv\"\n",
    "    qc_df.to_csv(qc_fp, index=False)\n",
    "    log(f\"Wrote: {qc_fp}\")\n",
    "\n",
    "    # README deliverable\n",
    "    readme = f\"\"\"# IFCB ↔ PACE OCI L3M Rrs Matchup Deliverable\n",
    "\n",
    "## What this deliverable contains\n",
    "- **matchups_features_wide.csv**: ML-ready features table (Rrs at many wavelengths as columns).\n",
    "- **matchups_features_long.csv**: tidy long-format table (one row per wavelength).\n",
    "- **ifcb_labels_daily.csv**: daily IFCB labels aggregated into coarse phytoplankton groups (counts + fractions).\n",
    "- **matchups_with_labels.csv**: merged features + labels for model training.\n",
    "- **montage_rrs_{int(round(target_nm))}nm.png**: daily montage map around the IFCB location.\n",
    "- **spectra_*.png**: Rrs spectrum plot per date/location.\n",
    "- **qc_summary.csv**: summary stats + configuration.\n",
    "- **log.txt**: run log.\n",
    "\n",
    "## PACE source\n",
    "- Product short_name used: **{pace_short_name}**\n",
    "- Matchup method: nearest-neighbor pixel in the daily L3M mapped product.\n",
    "\n",
    "## Important note on spatial resolution\n",
    "This dataset uses **L3M**. Typical L3M resolutions may be ~4 km (product-dependent).  \n",
    "If the downstream goal is ~1 km coastal community maps, consider switching to **L2 swath** or a finer-resolution gridded product.\n",
    "\n",
    "## Label definition\n",
    "IFCB class/taxon labels were mapped into coarse groups via regex rules in:\n",
    "`default_group_mapping()` inside the script.\n",
    "\n",
    "## Reproducibility\n",
    "- Requires Earthdata login (`earthaccess login`)\n",
    "- Dependencies pinned in `requirements.txt`\n",
    "\"\"\"\n",
    "    write_text(outp / \"README_deliverable.md\", readme)\n",
    "    log(\"Wrote: README_deliverable.md\")\n",
    "\n",
    "    # requirements\n",
    "    req = \"\"\"earthaccess\n",
    "xarray\n",
    "netcdf4\n",
    "h5netcdf\n",
    "pandas\n",
    "numpy\n",
    "matplotlib\n",
    "cartopy\n",
    "\"\"\"\n",
    "    write_text(outp / \"requirements.txt\", req)\n",
    "    log(\"Wrote: requirements.txt\")\n",
    "\n",
    "    log(\"DONE.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    IFCB_CSV = os.environ.get(\"IFCB_CSV\", \"pace-flow/DATA_TEST/ifcb.csv\")\n",
    "    OUT_DIR = os.environ.get(\"OUT_DIR\", \"outputs\")\n",
    "    PACE_SHORTNAME = os.environ.get(\"PACE_SHORTNAME\", \"PACE_OCI_L3M_RRS\")\n",
    "\n",
    "    TARGET_NM = float(os.environ.get(\"TARGET_NM\", \"442\"))\n",
    "    MONTAGE_HALF_WINDOW_DEG = float(os.environ.get(\"MONTAGE_HALF_WINDOW_DEG\", \"1.0\"))\n",
    "    MONTAGE_NCOLS = int(os.environ.get(\"MONTAGE_NCOLS\", \"4\"))\n",
    "\n",
    "    main(\n",
    "        ifcb_csv=IFCB_CSV,\n",
    "        out_dir=OUT_DIR,\n",
    "        pace_short_name=PACE_SHORTNAME,\n",
    "        target_nm=TARGET_NM,\n",
    "        montage_half_window_deg=MONTAGE_HALF_WINDOW_DEG,\n",
    "        montage_ncols=MONTAGE_NCOLS,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
