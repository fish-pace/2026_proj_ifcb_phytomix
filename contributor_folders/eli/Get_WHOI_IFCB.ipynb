{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2315ab36-373a-4418-9841-d6fdefa0f159",
   "metadata": {},
   "source": [
    "# Get IFCB data from WHOI dashboard\n",
    "\n",
    "* https://ifcb-data.whoi.edu\n",
    "* https://ifcb.caloos.org/dashboard\n",
    "* https://github.com/hsosik/ifcb-analysis/wik\n",
    "\n",
    "## What is IFCB data?\n",
    "\n",
    "The **Imaging FlowCytobot (IFCB)** is an instrument that continuously samples seawater and takes images of individual particles as they flow past a camera. Each detected particle is saved as a small image called a **Region of Interest (ROI)**. An ROI usually represents a single phytoplankton cell, zooplankton, detritus particle, or other object in the water.\n",
    "\n",
    "For each sampling period (called a **bin**), IFCB records thousands of ROIs along with metadata describing when, where, and how much water was analyzed. Automated classifiers are often applied to the ROI images, producing a table of **class scores** that estimate how likely each ROI belongs to different species or functional groups.\n",
    "\n",
    "---\n",
    "\n",
    "## How do we compute abundance (objects per mL)?\n",
    "\n",
    "Abundance is calculated by combining **object counts** with the **volume of water analyzed**:\n",
    "\n",
    "1. **Count objects**  \n",
    "   Each row in a `*_class_scores.csv` file corresponds to **one ROI (one detected object)**.  \n",
    "   To estimate species-level counts, each ROI is assigned to the class with the highest classification score (“winner”), sometimes requiring the score to exceed a confidence threshold.\n",
    "\n",
    "2. **Get analyzed volume**  \n",
    "   For each bin, IFCB metadata includes volume analyzed (`ml_analyzed` in our dataset), the total volume of seawater (in milliliters) that passed the detector during that sample.\n",
    "\n",
    "3. **Compute abundance**  \n",
    "\n",
    "   $$\n",
    "   \\text{objects per mL} = \\frac{\\text{number of ROIs (or ROIs of a given species)}}{\\text{ml\\_analyzed}}\n",
    "   $$\n",
    "\n",
    "This converts image-based counts into a physically meaningful concentration. Abundance per mL.\n",
    "\n",
    "## FCB files per bin\n",
    "\n",
    "For a given bin `DYYYYMMDDTHHMMSS_IFCBXXX`, we have:\n",
    "\n",
    "* *_class_scores.csv\n",
    "    - One row per ROI (detected object)\n",
    "    - Columns = classifier scores (probabilities)\n",
    "\n",
    "* *_features.csv\n",
    "    - One row per ROI (object)\n",
    "    - Contains morphological / size features, e.g.: area, equivalent spherical diameter (ESD), major/minor axis, perimeter, biovolume\n",
    "\n",
    "This notebook is just using the class_scores.csv file.\n",
    "\n",
    "## Things to be aware of\n",
    "\n",
    "* Look at the size drop-off. Sizes in features files for each ROI.\n",
    "* Colony forming\n",
    "\n",
    "## Look at the metadata first\n",
    "\n",
    "We see that we have the ml analyzed and the number of objects identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdf61349-b261-45a3-aa4c-90ef33b1e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full metadata (first ~2000 chars):\n",
      "\n",
      "{\n",
      "  \"scale\": 0.33,\n",
      "  \"shape\": [\n",
      "    600,\n",
      "    800\n",
      "  ],\n",
      "  \"previous_bin_id\": \"D20240215T143741_IFCB010\",\n",
      "  \"next_bin_id\": \"D20240215T152410_IFCB010\",\n",
      "  \"lat\": 41.325,\n",
      "  \"lng\": -70.5667,\n",
      "  \"lat_rounded\": \"41.325\",\n",
      "  \"lng_rounded\": \"-70.5667\",\n",
      "  \"depth\": 4.0,\n",
      "  \"pages\": [\n",
      "    0,\n",
      "    1\n",
      "  ],\n",
      "  \"num_pages\": 1,\n",
      "  \"tags\": [],\n",
      "  \"coordinates\": [],\n",
      "  \"has_blobs\": false,\n",
      "  \"has_features\": false,\n",
      "  \"has_class_scores\": false,\n",
      "  \"timestamp_iso\": \"2024-02-15T15:00:55+00:00\",\n",
      "  \"instrument\": \"IFCB10\",\n",
      "  \"num_triggers\": 7031,\n",
      "  \"num_images\": 6521,\n",
      "  \"trigger_freq\": 5.874,\n",
      "  \"ml_analyzed\": \"2.851 ml\",\n",
      "  \"size\": 56579281,\n",
      "  \"datasets\": [\n",
      "    \"mvco\"\n",
      "  ],\n",
      "  \"primary_dataset\": \"mvco\",\n",
      "  \"comments\": [],\n",
      "  \"concentration\": 2287.189,\n",
      "  \"skip\": false,\n",
      "  \"sample_type\": \"\",\n",
      "  \"cruise\": \"\",\n",
      "  \"cast\": \"\",\n",
      "  \"niskin\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = \"https://ifcb-data.whoi.edu\"\n",
    "dataset = \"mvco\"\n",
    "bin_id = \"D20240215T150055_IFCB010\"  # pick one bin you know exists\n",
    "\n",
    "url = f\"{base_url}/api/bin/{bin_id}\"\n",
    "params = {\"dataset\": dataset}\n",
    "\n",
    "r = requests.get(url, params=params, timeout=30)\n",
    "r.raise_for_status()\n",
    "\n",
    "data = r.json()\n",
    "\n",
    "# Pretty-print the whole JSON\n",
    "print(\"\\nFull metadata (first ~2000 chars):\\n\")\n",
    "print(json.dumps(data, indent=2)[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1567fd1b-e87a-44b4-ba70-233974e7dd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>Acanthoica_quattrospina</th>\n",
       "      <th>Akashiwo</th>\n",
       "      <th>Alexandrium_catenella</th>\n",
       "      <th>Amphidinium</th>\n",
       "      <th>Amylax</th>\n",
       "      <th>Apedinella</th>\n",
       "      <th>Asterionellopsis_glacialis</th>\n",
       "      <th>Bacillaria</th>\n",
       "      <th>Bacillariophyceae</th>\n",
       "      <th>...</th>\n",
       "      <th>nanoplankton_mix</th>\n",
       "      <th>pennate</th>\n",
       "      <th>pennate_Pseudo-nitzschia</th>\n",
       "      <th>pennate_Thalassionema</th>\n",
       "      <th>pennate_morphotype1</th>\n",
       "      <th>pollen</th>\n",
       "      <th>shellfish_larvae</th>\n",
       "      <th>Bacillariophyceae_morphotype1</th>\n",
       "      <th>unknown2</th>\n",
       "      <th>zooplankton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D20240215T150055_IFCB010_00001</td>\n",
       "      <td>6.000000e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000e-08</td>\n",
       "      <td>6.000000e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.300000e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>7.495000e-01</td>\n",
       "      <td>2.044000e-05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D20240215T150055_IFCB010_00002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D20240215T150055_IFCB010_00003</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.850000e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.400000e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e-07</td>\n",
       "      <td>6.600000e-07</td>\n",
       "      <td>3.000000e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D20240215T150055_IFCB010_00004</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D20240215T150055_IFCB010_00005</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.217000e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pid  Acanthoica_quattrospina  Akashiwo  \\\n",
       "0  D20240215T150055_IFCB010_00001             6.000000e-08       0.0   \n",
       "1  D20240215T150055_IFCB010_00002             0.000000e+00       0.0   \n",
       "2  D20240215T150055_IFCB010_00003             0.000000e+00       0.0   \n",
       "3  D20240215T150055_IFCB010_00004             0.000000e+00       0.0   \n",
       "4  D20240215T150055_IFCB010_00005             0.000000e+00       0.0   \n",
       "\n",
       "   Alexandrium_catenella  Amphidinium  Amylax    Apedinella  \\\n",
       "0                    0.0          0.0     0.0  6.000000e-08   \n",
       "1                    0.0          0.0     0.0  0.000000e+00   \n",
       "2                    0.0          0.0     0.0  0.000000e+00   \n",
       "3                    0.0          0.0     0.0  0.000000e+00   \n",
       "4                    0.0          0.0     0.0  0.000000e+00   \n",
       "\n",
       "   Asterionellopsis_glacialis  Bacillaria  Bacillariophyceae  ...  \\\n",
       "0                6.000000e-08         0.0       8.300000e-07  ...   \n",
       "1                0.000000e+00         0.0       0.000000e+00  ...   \n",
       "2                1.850000e-06         0.0       5.400000e-07  ...   \n",
       "3                3.000000e-07         0.0       0.000000e+00  ...   \n",
       "4                1.217000e-04         0.0       0.000000e+00  ...   \n",
       "\n",
       "   nanoplankton_mix       pennate  pennate_Pseudo-nitzschia  \\\n",
       "0      7.495000e-01  2.044000e-05              0.000000e+00   \n",
       "1      0.000000e+00  0.000000e+00              0.000000e+00   \n",
       "2      0.000000e+00  3.000000e-07              6.600000e-07   \n",
       "3      6.000000e-08  0.000000e+00              0.000000e+00   \n",
       "4      0.000000e+00  6.000000e-08              0.000000e+00   \n",
       "\n",
       "   pennate_Thalassionema  pennate_morphotype1  pollen  shellfish_larvae  \\\n",
       "0           0.000000e+00                  0.0     0.0               0.0   \n",
       "1           0.000000e+00                  0.0     0.0               0.0   \n",
       "2           3.000000e-07                  0.0     0.0               0.0   \n",
       "3           0.000000e+00                  0.0     0.0               0.0   \n",
       "4           0.000000e+00                  0.0     0.0               0.0   \n",
       "\n",
       "   Bacillariophyceae_morphotype1      unknown2  zooplankton  \n",
       "0                   0.000000e+00  0.000000e+00          0.0  \n",
       "1                   0.000000e+00  0.000000e+00          0.0  \n",
       "2                   0.000000e+00  0.000000e+00          0.0  \n",
       "3                   6.000000e-08  0.000000e+00          0.0  \n",
       "4                   0.000000e+00  5.000000e-07          0.0  \n",
       "\n",
       "[5 rows x 156 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at one bin file\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "bin_id = \"D20240215T150055_IFCB010\"\n",
    "url = f\"https://ifcb-data.whoi.edu/mvco/{bin_id}_class_scores.csv\"\n",
    "\n",
    "r = requests.get(url)\n",
    "r.raise_for_status()\n",
    "\n",
    "df = pd.read_csv(io.StringIO(r.text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24784111-c815-4d8f-aff0-e1f959baf8ad",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "These process bin files in parallel, compute abundance (objects per ml) for each bin, and then summarize that into an average abundance per day by combining all the bins in a single day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a8e833-e911-420b-b025-f444e7f38110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def summarize_bins_to_daily_csvs_streaming_parallel(\n",
    "    start_bin_id,\n",
    "    end_bin_id,\n",
    "    base_url,\n",
    "    dataset,\n",
    "    instrument, # if None -> no filtering; if set -> filter traversal\n",
    "    output_dir,\n",
    "    thresh=0.0, # thresh is 0 to 1. 0 is winner takes all\n",
    "    max_workers=8,\n",
    "    vol_label=\"ml_analyzed\",  # e.g. \"ml_analyzed\" -> \"2.851 ml\"\n",
    "    lat_label=\"lat\",\n",
    "    lon_label=\"lng\",\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    _tls = threading.local()\n",
    "\n",
    "    def get_session():\n",
    "        if getattr(_tls, \"session\", None) is None:\n",
    "            s = requests.Session()\n",
    "            adapter = requests.adapters.HTTPAdapter(\n",
    "                pool_connections=4,\n",
    "                pool_maxsize=4,\n",
    "                max_retries=2,\n",
    "            )\n",
    "            s.mount(\"https://\", adapter)\n",
    "            s.mount(\"http://\", adapter)\n",
    "            _tls.session = s\n",
    "        return _tls.session\n",
    "\n",
    "    def parse_ml(d):\n",
    "        raw = d.get(vol_label)\n",
    "        if raw is None:\n",
    "            return None\n",
    "        try:\n",
    "            return float(raw.split()[0]) if isinstance(raw, str) else float(raw)\n",
    "        except (TypeError, ValueError):\n",
    "            return None\n",
    "\n",
    "    def fetch_bin_meta(bin_id):\n",
    "        url = f\"{base_url}/api/bin/{bin_id}\"\n",
    "        params = {\"dataset\": dataset}\n",
    "        if instrument is not None:\n",
    "            params[\"instrument\"] = instrument\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return None, None\n",
    "        d = r.json()\n",
    "        return d, d.get(\"next_bin_id\")\n",
    "\n",
    "    def fetch_and_summarize_bin(bin_id, vol_ml):\n",
    "        s = get_session()\n",
    "        url = f\"{base_url}/{dataset}/{bin_id}_class_scores.csv\"\n",
    "        r = s.get(url, timeout=60)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        df = pd.read_csv(io.StringIO(r.text))\n",
    "        if df is None or df.empty or \"pid\" not in df.columns:\n",
    "            return None\n",
    "\n",
    "        class_cols = [c for c in df.columns if c != \"pid\"]\n",
    "        scores = df[class_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(float(\"-inf\"))\n",
    "        winner = scores.idxmax(axis=1)\n",
    "        max_score = scores.max(axis=1)\n",
    "\n",
    "        keep = max_score >= thresh\n",
    "        counts = winner[keep].value_counts().to_dict()  # may be empty\n",
    "        return (vol_ml, counts)\n",
    "\n",
    "    def flush_day(date, ml_total, counts, lat, lon, inst):\n",
    "        if date is None:\n",
    "            return None\n",
    "        if ml_total <= 0 or not counts:\n",
    "            return None\n",
    "\n",
    "        rows = []\n",
    "        for cls, ct in sorted(counts.items()):\n",
    "            per_ml = ct / ml_total\n",
    "            rows.append({\n",
    "                \"date\": date,\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lon,\n",
    "                \"instrument\": inst,\n",
    "                \"class\": cls,\n",
    "                \"count\": int(ct),\n",
    "                \"ml_analyzed_total\": float(ml_total),\n",
    "                \"objects_per_ml\": float(per_ml),\n",
    "                \"objects_per_L\": float(per_ml * 1000),\n",
    "                \"threshold\": thresh,\n",
    "            })\n",
    "\n",
    "        out = pd.DataFrame(rows)\n",
    "        out_file = os.path.join(output_dir, f\"{date}_daily_summary_long.csv\")\n",
    "        out.to_csv(out_file, index=False)\n",
    "        return out_file\n",
    "\n",
    "    written = []\n",
    "\n",
    "    current_date = None\n",
    "    day_lat = None\n",
    "    day_lon = None\n",
    "    day_instrument = None\n",
    "    futures = []\n",
    "\n",
    "    n_meta = 0\n",
    "    pbar = tqdm(desc=\"Streaming metadata\", unit=\"bin\")\n",
    "\n",
    "    current_bin_id = start_bin_id\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        while True:\n",
    "            d, next_bin_id = fetch_bin_meta(current_bin_id)\n",
    "            if d is None:\n",
    "                print(f\"Metadata fetch failed for {current_bin_id}. Stopping.\")\n",
    "                break\n",
    "\n",
    "            bin_id = current_bin_id\n",
    "            date = bin_id.split(\"T\")[0][1:]  # YYYYMMDD\n",
    "            vol_ml = parse_ml(d)\n",
    "\n",
    "            # day change -> collect futures, write previous day, reset\n",
    "            if current_date is not None and date != current_date:\n",
    "                ml_total = 0.0\n",
    "                counts = {}\n",
    "                for fut in as_completed(futures):\n",
    "                    res = fut.result()\n",
    "                    if res is None:\n",
    "                        continue\n",
    "                    v, cdict = res\n",
    "\n",
    "                    # ---- FIX: ALWAYS add volume for valid bins ----\n",
    "                    ml_total += float(v)\n",
    "\n",
    "                    # counts only if any ROIs passed threshold\n",
    "                    for cls, ct in cdict.items():\n",
    "                        counts[cls] = counts.get(cls, 0) + int(ct)\n",
    "\n",
    "                out_file = flush_day(current_date, ml_total, counts, day_lat, day_lon, day_instrument)\n",
    "                if out_file:\n",
    "                    written.append(out_file)\n",
    "                    tqdm.write(f\"Wrote: {out_file}\")\n",
    "\n",
    "                futures = []\n",
    "                day_lat = None\n",
    "                day_lon = None\n",
    "                day_instrument = None\n",
    "\n",
    "            current_date = date\n",
    "\n",
    "            if day_lat is None:\n",
    "                day_lat = d.get(lat_label)\n",
    "            if day_lon is None:\n",
    "                day_lon = d.get(lon_label)\n",
    "            if day_instrument is None:\n",
    "                day_instrument = d.get(\"instrument\")\n",
    "\n",
    "            if vol_ml and vol_ml > 0:\n",
    "                futures.append(executor.submit(fetch_and_summarize_bin, bin_id, vol_ml))\n",
    "\n",
    "            n_meta += 1\n",
    "            pbar.update(1)\n",
    "            if n_meta % 200 == 0:\n",
    "                pbar.set_postfix_str(f\"day={date} queued={len(futures)}\")\n",
    "\n",
    "            if bin_id == end_bin_id:\n",
    "                break\n",
    "\n",
    "            if not next_bin_id:\n",
    "                break\n",
    "            current_bin_id = next_bin_id\n",
    "\n",
    "        # flush last day\n",
    "        if current_date is not None:\n",
    "            ml_total = 0.0\n",
    "            counts = {}\n",
    "            for fut in as_completed(futures):\n",
    "                res = fut.result()\n",
    "                if res is None:\n",
    "                    continue\n",
    "                v, cdict = res\n",
    "\n",
    "                # ---- FIX: ALWAYS add volume for valid bins ----\n",
    "                ml_total += float(v)\n",
    "\n",
    "                for cls, ct in cdict.items():\n",
    "                    counts[cls] = counts.get(cls, 0) + int(ct)\n",
    "\n",
    "            out_file = flush_day(current_date, ml_total, counts, day_lat, day_lon, day_instrument)\n",
    "            if out_file:\n",
    "                written.append(out_file)\n",
    "                tqdm.write(f\"Wrote: {out_file}\")\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"Done. Wrote {len(written)} daily files to {output_dir}\")\n",
    "    return written\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf2a2e-165e-4721-8601-d7d81eb0cbf4",
   "metadata": {},
   "source": [
    "## Run the files\n",
    "\n",
    "You need to look up the starting and ending bin_id from the dashboard. For example, https://ifcb-data.whoi.edu/timeline?dataset=mvco, roll over the timeline to see bin_id. Look up the instrument from a bin file on the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b2fe1dc-d693-4412-a9fb-a547bb378105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09e3882a2274b77a2b526e8afa01a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Streaming metadata: 0bin [00:00, ?bin/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ../../data/mvco/daily_summaries/20250704_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250705_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250706_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250707_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250708_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250709_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250710_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250711_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250712_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250713_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250714_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250715_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250716_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250717_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250718_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250719_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250720_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250721_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250722_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250723_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250724_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250725_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250726_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250727_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250728_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250729_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250730_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250731_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250801_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250802_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250803_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250804_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250805_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250806_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250807_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250808_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250809_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250810_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250811_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250812_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250813_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250814_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250815_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250816_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250817_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250818_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250819_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250820_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250821_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250822_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250823_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250824_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250825_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250826_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250827_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250828_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250829_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250830_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250831_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250901_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250902_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250903_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250904_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250905_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250906_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250907_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250908_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250909_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250910_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250911_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250912_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250913_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250914_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250915_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250916_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250917_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250918_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250919_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250920_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250921_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250922_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250923_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250924_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250925_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250926_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250927_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250928_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250929_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20250930_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251001_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251002_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251003_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251004_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251005_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251006_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251007_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251008_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251009_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251010_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251011_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251012_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251013_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251014_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251015_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251016_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251017_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251018_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251019_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251020_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251021_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251022_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251023_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251024_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251025_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251026_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251027_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251028_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251029_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251030_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251031_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251101_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251102_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251103_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251104_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251105_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251106_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251107_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251108_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251109_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251110_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251111_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251112_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251113_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251114_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251115_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251116_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251117_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251118_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251119_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251120_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251121_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251122_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251123_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251124_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251125_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251126_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251127_daily_summary_long.csv\n",
      "Wrote: ../../data/mvco/daily_summaries/20251128_daily_summary_long.csv\n",
      "Done. Wrote 148 daily files to ../../data/mvco/daily_summaries\n"
     ]
    }
   ],
   "source": [
    "# Winner-takes-all:\n",
    "# - If all class scores are in [0, 1], then yes: set thresh=0.0 and you keep every ROI.\n",
    "# - If scores can be negative / -inf, safer is thresh=float(\"-inf\") to truly keep all ROIs.\n",
    "\n",
    "written_files = summarize_bins_to_daily_csvs_streaming_parallel(\n",
    "#    start_bin_id=\"D20240215T150055_IFCB010\",\n",
    "#    start_bin_id=\"D20240630T001941_IFCB127\",\n",
    "#    start_bin_id=\"D20250101T000139_IFCB010\",\n",
    "    start_bin_id=\"D20250704T001617_IFCB127\",\n",
    "    end_bin_id = \"D20251205T204355_IFCB010\",\n",
    "    base_url=\"https://ifcb-data.whoi.edu\",\n",
    "    dataset=\"mvco\",\n",
    "    instrument=None,\n",
    "    output_dir=\"../../data/mvco/daily_summaries\",\n",
    "    thresh=0.0,\n",
    "    max_workers=8,  # try 8; if rate-limited, drop to 4–6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43084264-dd25-46c9-b1d4-b7415f7d24a5",
   "metadata": {},
   "source": [
    "## Merge into one summary file with all days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f15c6e09-c37a-4ffc-9dbd-0dd0ece5865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 399 daily files\n",
      "Wrote wide file: ../../data/mvco_objects_per_ml.csv\n",
      "        date  latitude  longitude  threshold  ml_analyzed_total  Acantharia  \\\n",
      "0 2024-02-15    41.325   -70.5667        0.0             67.990    0.000000   \n",
      "1 2024-02-16    41.325   -70.5667        0.0            126.667    0.000000   \n",
      "2 2024-02-17    41.325   -70.5667        0.0            120.524    0.008297   \n",
      "3 2024-02-18    41.325   -70.5667        0.0            101.251    0.009876   \n",
      "4 2024-02-19    41.325   -70.5667        0.0             87.598    0.022832   \n",
      "\n",
      "   Acanthoica_quattrospina  Akashiwo  Alexandrium_catenella    Amoeba  ...  \\\n",
      "0                 7.824680       0.0               0.250037  2.426827  ...   \n",
      "1                 5.668406       0.0               0.513157  5.731564  ...   \n",
      "2                 6.355581       0.0               0.365073  5.476088  ...   \n",
      "3                 5.560439       0.0               0.325923  7.713504  ...   \n",
      "4                 4.680472       0.0               0.228316  8.573255  ...   \n",
      "\n",
      "   flagellate_morphotype3  nanoplankton_mix    pennate  \\\n",
      "0                8.221797        559.155758   3.574055   \n",
      "1               14.431541        709.134976   6.252615   \n",
      "2               17.100329        913.768212   7.077429   \n",
      "3               19.673880       1084.364599   8.355473   \n",
      "4               23.425192       1301.079933  10.742254   \n",
      "\n",
      "   pennate_Pseudo-nitzschia  pennate_Thalassionema  pennate_morphotype1  \\\n",
      "0                  0.191205               0.191205             0.014708   \n",
      "1                  0.228947               0.449999             0.039474   \n",
      "2                  0.132754               0.248913             0.000000   \n",
      "3                  0.098764               0.493822             0.039506   \n",
      "4                  0.114158               0.262563             0.045663   \n",
      "\n",
      "     pollen  shellfish_larvae  unknown2  zooplankton  \n",
      "0  0.000000               0.0  0.294161     0.014708  \n",
      "1  0.039474               0.0  0.986840     0.000000  \n",
      "2  0.008297               0.0  0.497826     0.008297  \n",
      "3  0.049382               0.0  0.859251     0.000000  \n",
      "4  0.011416               0.0  0.662116     0.000000  \n",
      "\n",
      "[5 rows x 160 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Where your daily files are\n",
    "daily_dir = \"../../data/mvco/daily_summaries\"\n",
    "out_file  = \"../../data/mvco_objects_per_ml.csv\"\n",
    "\n",
    "# Find all daily summaries\n",
    "files = sorted(glob.glob(os.path.join(daily_dir, \"*_daily_summary_long.csv\")))\n",
    "print(f\"Found {len(files)} daily files\")\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    # Keep only what we need for the wide pivot\n",
    "    df = df[[\"date\", \"latitude\", \"longitude\", \"threshold\", \"ml_analyzed_total\", \"class\", \"objects_per_ml\"]]\n",
    "    dfs.append(df)\n",
    "\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ---- types ----\n",
    "merged[\"date\"] = pd.to_datetime(merged[\"date\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "merged[\"threshold\"] = pd.to_numeric(merged[\"threshold\"], errors=\"coerce\")\n",
    "merged[\"ml_analyzed_total\"] = pd.to_numeric(merged[\"ml_analyzed_total\"], errors=\"coerce\")\n",
    "merged[\"objects_per_ml\"] = pd.to_numeric(merged[\"objects_per_ml\"], errors=\"coerce\")\n",
    "\n",
    "# ---- pivot to wide ----\n",
    "# One row per day/lat/lon/threshold/volume; one column per class; values = objects_per_ml\n",
    "wide = (\n",
    "    merged.pivot_table(\n",
    "        index=[\"date\", \"latitude\", \"longitude\", \"threshold\", \"ml_analyzed_total\"],\n",
    "        columns=\"class\",\n",
    "        values=\"objects_per_ml\",\n",
    "        aggfunc=\"sum\",     # should be unique, but sum is safe if duplicates exist\n",
    "        fill_value=0.0,\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Optional: flatten the column index name and sort columns nicely\n",
    "wide.columns.name = None\n",
    "meta_cols = [\"date\", \"latitude\", \"longitude\", \"threshold\", \"ml_analyzed_total\"]\n",
    "class_cols = sorted([c for c in wide.columns if c not in meta_cols])\n",
    "wide = wide[meta_cols + class_cols]\n",
    "\n",
    "os.makedirs(os.path.dirname(out_file), exist_ok=True)\n",
    "wide.to_csv(out_file, index=False)\n",
    "\n",
    "print(f\"Wrote wide file: {out_file}\")\n",
    "print(wide.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd43e79-5994-4e9c-999f-3d1a53d99309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
