{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2315ab36-373a-4418-9841-d6fdefa0f159",
   "metadata": {},
   "source": [
    "# Get IFCB data from WHOI dashboard\n",
    "\n",
    "* https://ifcb-data.whoi.edu\n",
    "* https://ifcb.caloos.org/dashboard\n",
    "* https://github.com/hsosik/ifcb-analysis/wik\n",
    "\n",
    "## What is IFCB data?\n",
    "\n",
    "The **Imaging FlowCytobot (IFCB)** is an instrument that continuously samples seawater and takes images of individual particles as they flow past a camera. Each detected particle is saved as a small image called a **Region of Interest (ROI)**. An ROI usually represents a single phytoplankton cell, zooplankton, detritus particle, or other object in the water.\n",
    "\n",
    "For each sampling period (called a **bin**), IFCB records thousands of ROIs along with metadata describing when, where, and how much water was analyzed. Automated classifiers are often applied to the ROI images, producing a table of **class scores** that estimate how likely each ROI belongs to different species or functional groups.\n",
    "\n",
    "---\n",
    "\n",
    "## How do we compute abundance (objects per mL)?\n",
    "\n",
    "Abundance is calculated by combining **object counts** with the **volume of water analyzed**:\n",
    "\n",
    "1. **Count objects**  \n",
    "   Each row in a `*_class_scores.csv` file corresponds to **one ROI (one detected object)**.  \n",
    "   To estimate species-level counts, each ROI is assigned to the class with the highest classification score (“winner”), often requiring the score to exceed a confidence threshold.\n",
    "\n",
    "2. **Get analyzed volume**  \n",
    "   For each bin, IFCB metadata includes volume analyzed (`ml_analyzed` in our dataset), the total volume of seawater (in milliliters) that passed the detector during that sample.\n",
    "\n",
    "3. **Compute abundance**  \n",
    "\n",
    "   $$\n",
    "   \\text{objects per mL} = \\frac{\\text{number of ROIs (or ROIs of a given species)}}{\\text{ml\\_analyzed}}\n",
    "   $$\n",
    "\n",
    "This converts image-based counts into a physically meaningful concentration. Abundance per mL.\n",
    "\n",
    "## FCB files per bin\n",
    "\n",
    "For a given bin `DYYYYMMDDTHHMMSS_IFCBXXX`, we have:\n",
    "\n",
    "* *_class_scores.csv\n",
    "    - One row per ROI (detected object)\n",
    "    - Columns = classifier scores (probabilities)\n",
    "\n",
    "* *_features.csv\n",
    "    - One row per ROI (object)\n",
    "    - Contains morphological / size features, e.g.: area, equivalent spherical diameter (ESD), major/minor axis, perimeter, biovolume\n",
    "\n",
    "This notebook is just using the class_scores.csv file.\n",
    "\n",
    "## Things to be aware of\n",
    "\n",
    "* Look at the size drop-off. Sizes in features files for each ROI.\n",
    "* Colony forming\n",
    "\n",
    "## Look at the metadata first\n",
    "\n",
    "We see that we have the ml analyzed and the number of objects identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdf61349-b261-45a3-aa4c-90ef33b1e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full metadata (first ~2000 chars):\n",
      "\n",
      "{\n",
      "  \"scale\": 0.33,\n",
      "  \"shape\": [\n",
      "    600,\n",
      "    800\n",
      "  ],\n",
      "  \"previous_bin_id\": \"D20240215T143741_IFCB010\",\n",
      "  \"next_bin_id\": \"D20240215T152410_IFCB010\",\n",
      "  \"lat\": 41.325,\n",
      "  \"lng\": -70.5667,\n",
      "  \"lat_rounded\": \"41.325\",\n",
      "  \"lng_rounded\": \"-70.5667\",\n",
      "  \"depth\": 4.0,\n",
      "  \"pages\": [\n",
      "    0,\n",
      "    1\n",
      "  ],\n",
      "  \"num_pages\": 1,\n",
      "  \"tags\": [],\n",
      "  \"coordinates\": [],\n",
      "  \"has_blobs\": false,\n",
      "  \"has_features\": false,\n",
      "  \"has_class_scores\": false,\n",
      "  \"timestamp_iso\": \"2024-02-15T15:00:55+00:00\",\n",
      "  \"instrument\": \"IFCB10\",\n",
      "  \"num_triggers\": 7031,\n",
      "  \"num_images\": 6521,\n",
      "  \"trigger_freq\": 5.874,\n",
      "  \"ml_analyzed\": \"2.851 ml\",\n",
      "  \"size\": 56579281,\n",
      "  \"datasets\": [\n",
      "    \"mvco\"\n",
      "  ],\n",
      "  \"primary_dataset\": \"mvco\",\n",
      "  \"comments\": [],\n",
      "  \"concentration\": 2287.189,\n",
      "  \"skip\": false,\n",
      "  \"sample_type\": \"\",\n",
      "  \"cruise\": \"\",\n",
      "  \"cast\": \"\",\n",
      "  \"niskin\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = \"https://ifcb-data.whoi.edu\"\n",
    "dataset = \"mvco\"\n",
    "bin_id = \"D20240215T150055_IFCB010\"  # pick one bin you know exists\n",
    "\n",
    "url = f\"{base_url}/api/bin/{bin_id}\"\n",
    "params = {\"dataset\": dataset}\n",
    "\n",
    "r = requests.get(url, params=params, timeout=30)\n",
    "r.raise_for_status()\n",
    "\n",
    "data = r.json()\n",
    "\n",
    "# Pretty-print the whole JSON\n",
    "print(\"\\nFull metadata (first ~2000 chars):\\n\")\n",
    "print(json.dumps(data, indent=2)[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24784111-c815-4d8f-aff0-e1f959baf8ad",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "These process bin files in parallel, compute abundance (objects per ml) for each bin, and then summarize that into an average abundance per day by combining all the bins in a single day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a8e833-e911-420b-b025-f444e7f38110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def summarize_bins_to_daily_csvs_streaming_parallel(\n",
    "    start_bin_id,\n",
    "    end_bin_id,\n",
    "    base_url,\n",
    "    dataset,\n",
    "    instrument,          # if None -> no filtering; if set -> filter traversal\n",
    "    output_dir,\n",
    "    thresh=0.7,\n",
    "    max_workers=8,\n",
    "    vol_label=\"ml_analyzed\",  # e.g. \"ml_analyzed\" -> \"2.851 ml\"\n",
    "    lat_label=\"lat\",\n",
    "    lon_label=\"lng\",\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ---- thread-local Session (thread-safe reuse of connections) ----\n",
    "    _tls = threading.local()\n",
    "\n",
    "    def get_session():\n",
    "        if getattr(_tls, \"session\", None) is None:\n",
    "            s = requests.Session()\n",
    "            adapter = requests.adapters.HTTPAdapter(\n",
    "                pool_connections=4,\n",
    "                pool_maxsize=4,\n",
    "                max_retries=2,\n",
    "            )\n",
    "            s.mount(\"https://\", adapter)\n",
    "            s.mount(\"http://\", adapter)\n",
    "            _tls.session = s\n",
    "        return _tls.session\n",
    "\n",
    "    def parse_ml(d):\n",
    "        raw = d.get(vol_label)\n",
    "        if raw is None:\n",
    "            return None\n",
    "        try:\n",
    "            return float(raw.split()[0]) if isinstance(raw, str) else float(raw)\n",
    "        except (TypeError, ValueError):\n",
    "            return None\n",
    "\n",
    "    def fetch_bin_meta(bin_id):\n",
    "        \"\"\"Serial: fetch metadata (also provides next_bin_id).\"\"\"\n",
    "        url = f\"{base_url}/api/bin/{bin_id}\"\n",
    "        params = {\"dataset\": dataset}\n",
    "        if instrument is not None:\n",
    "            params[\"instrument\"] = instrument   # <-- keep optional filtering\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return None, None\n",
    "        d = r.json()\n",
    "        return d, d.get(\"next_bin_id\")\n",
    "\n",
    "    def fetch_and_summarize_bin(bin_id, vol_ml):\n",
    "        \"\"\"Threaded: download class_scores and compute counts above thresh.\"\"\"\n",
    "        s = get_session()\n",
    "        url = f\"{base_url}/{dataset}/{bin_id}_class_scores.csv\"\n",
    "        r = s.get(url, timeout=60)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        df = pd.read_csv(io.StringIO(r.text))\n",
    "        if df is None or df.empty or \"pid\" not in df.columns:\n",
    "            return None\n",
    "\n",
    "        class_cols = [c for c in df.columns if c != \"pid\"]\n",
    "        scores = df[class_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(float(\"-inf\"))\n",
    "        winner = scores.idxmax(axis=1)\n",
    "        max_score = scores.max(axis=1)\n",
    "\n",
    "        keep = max_score >= thresh\n",
    "        if keep.sum() == 0:\n",
    "            return (vol_ml, {})\n",
    "\n",
    "        counts = winner[keep].value_counts().to_dict()\n",
    "        return (vol_ml, counts)\n",
    "\n",
    "    def flush_day(date, ml_total, counts, lat, lon, inst):\n",
    "        \"\"\"Write one day LONG-format CSV. Returns filepath or None.\"\"\"\n",
    "        if date is None:\n",
    "            return None\n",
    "        if ml_total <= 0 or not counts:\n",
    "            return None\n",
    "\n",
    "        rows = []\n",
    "        for cls, ct in sorted(counts.items()):\n",
    "            per_ml = ct / ml_total\n",
    "            rows.append({\n",
    "                \"date\": date,\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lon,\n",
    "                \"instrument\": inst,              # <-- recorded instrument (from metadata)\n",
    "                \"class\": cls,\n",
    "                \"count\": int(ct),\n",
    "                \"ml_analyzed_total\": float(ml_total),\n",
    "                \"objects_per_ml\": float(per_ml),\n",
    "                \"objects_per_L\": float(per_ml * 1000),\n",
    "                \"threshold\": thresh,\n",
    "            })\n",
    "\n",
    "        out = pd.DataFrame(rows)\n",
    "        out_file = os.path.join(output_dir, f\"{date}_daily_summary_long.csv\")\n",
    "        out.to_csv(out_file, index=False)\n",
    "        return out_file\n",
    "\n",
    "    written = []\n",
    "\n",
    "    current_date = None\n",
    "    day_lat = None\n",
    "    day_lon = None\n",
    "    day_instrument = None\n",
    "    futures = []\n",
    "\n",
    "    n_meta = 0\n",
    "    pbar = tqdm(desc=\"Streaming metadata\", unit=\"bin\")\n",
    "\n",
    "    current_bin_id = start_bin_id\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        while True:\n",
    "            d, next_bin_id = fetch_bin_meta(current_bin_id)\n",
    "            if d is None:\n",
    "                print(f\"Metadata fetch failed for {current_bin_id}. Stopping.\")\n",
    "                break\n",
    "\n",
    "            bin_id = current_bin_id\n",
    "\n",
    "            date = bin_id.split(\"T\")[0][1:]  # YYYYMMDD\n",
    "            vol_ml = parse_ml(d)\n",
    "\n",
    "            # day change -> collect futures, write previous day, reset\n",
    "            if current_date is not None and date != current_date:\n",
    "                ml_total = 0.0\n",
    "                counts = {}\n",
    "                for fut in as_completed(futures):\n",
    "                    res = fut.result()\n",
    "                    if res is None:\n",
    "                        continue\n",
    "                    v, cdict = res\n",
    "                    if cdict:\n",
    "                        ml_total += float(v)\n",
    "                        for cls, ct in cdict.items():\n",
    "                            counts[cls] = counts.get(cls, 0) + int(ct)\n",
    "\n",
    "                out_file = flush_day(current_date, ml_total, counts, day_lat, day_lon, day_instrument)\n",
    "                if out_file:\n",
    "                    written.append(out_file)\n",
    "                    tqdm.write(f\"Wrote: {out_file}\")\n",
    "\n",
    "                futures = []\n",
    "                day_lat = None\n",
    "                day_lon = None\n",
    "                day_instrument = None\n",
    "\n",
    "            current_date = date\n",
    "\n",
    "            # set day lat/lon/instrument from metadata (first bin of day)\n",
    "            if day_lat is None:\n",
    "                day_lat = d.get(lat_label)\n",
    "            if day_lon is None:\n",
    "                day_lon = d.get(lon_label)\n",
    "            if day_instrument is None:\n",
    "                day_instrument = d.get(\"instrument\")   # <-- always record actual instrument\n",
    "\n",
    "            if vol_ml and vol_ml > 0:\n",
    "                futures.append(executor.submit(fetch_and_summarize_bin, bin_id, vol_ml))\n",
    "\n",
    "            n_meta += 1\n",
    "            pbar.update(1)\n",
    "            if n_meta % 200 == 0:\n",
    "                pbar.set_postfix_str(f\"day={date} queued={len(futures)}\")\n",
    "\n",
    "            if bin_id == end_bin_id:\n",
    "                break\n",
    "\n",
    "            if not next_bin_id:\n",
    "                break\n",
    "            current_bin_id = next_bin_id\n",
    "\n",
    "        # flush last day\n",
    "        if current_date is not None:\n",
    "            ml_total = 0.0\n",
    "            counts = {}\n",
    "            for fut in as_completed(futures):\n",
    "                res = fut.result()\n",
    "                if res is None:\n",
    "                    continue\n",
    "                v, cdict = res\n",
    "                if cdict:\n",
    "                    ml_total += float(v)\n",
    "                    for cls, ct in cdict.items():\n",
    "                        counts[cls] = counts.get(cls, 0) + int(ct)\n",
    "\n",
    "            out_file = flush_day(current_date, ml_total, counts, day_lat, day_lon, day_instrument)\n",
    "            if out_file:\n",
    "                written.append(out_file)\n",
    "                tqdm.write(f\"Wrote: {out_file}\")\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"Done. Wrote {len(written)} daily files to {output_dir}\")\n",
    "    return written\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf2a2e-165e-4721-8601-d7d81eb0cbf4",
   "metadata": {},
   "source": [
    "## Run the files\n",
    "\n",
    "You need to look up the starting and ending bin_id from the dashboard. For example, https://ifcb-data.whoi.edu/timeline?dataset=mvco, roll over the timeline to see bin_id. Look up the instrument from a bin file on the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2fe1dc-d693-4412-a9fb-a547bb378105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2f516e9e524bbcb367ed4a68cf7d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Streaming metadata: 0bin [00:00, ?bin/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ../../data/mvco/daily_summaries/20240215_daily_summary_long.csv\n"
     ]
    }
   ],
   "source": [
    "written_files = summarize_bins_to_daily_csvs_streaming_parallel(\n",
    "    start_bin_id=\"D20240215T150055_IFCB010\",\n",
    "    end_bin_id = \"D20241227T181716_IFCB010\",\n",
    "    base_url=\"https://ifcb-data.whoi.edu\",\n",
    "    dataset=\"mvco\",\n",
    "    instrument=None,\n",
    "    output_dir=\"../../data/mvco/daily_summaries\",\n",
    "    thresh=0.7,\n",
    "    max_workers=8,  # try 8; if rate-limited, drop to 4–6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43084264-dd25-46c9-b1d4-b7415f7d24a5",
   "metadata": {},
   "source": [
    "## Merge into one summary file with all days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d731a81c-a0e9-44fa-9e98-dfff66536816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Where your daily files are\n",
    "daily_dir = \"../../data/mvco/daily_summaries\"\n",
    "out_file  = \"../../data/mvco/mvco_2024_abundance_long.csv\"\n",
    "\n",
    "# Find all daily summaries\n",
    "files = sorted(glob.glob(os.path.join(daily_dir, \"*_daily_summary_long.csv\")))\n",
    "print(f\"Found {len(files)} daily files\")\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    # Keep/rename to what you want\n",
    "    df = df[[\"date\", \"latitude\", \"longitude\", \"threshold\", \"ml_analyzed_total\", \"class\", \"objects_per_ml\"]]\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Optional: ensure types are sane\n",
    "merged[\"date\"] = pd.to_datetime(merged[\"day\"], format=\"%Y%m%d\")          # YYYY-MM-DD\n",
    "merged[\"threshold\"] = pd.to_numeric(merged[\"threshold\"], errors=\"coerce\")\n",
    "merged[\"ml_analyzed_total\"] = pd.to_numeric(merged[\"ml_analyzed_total\"], errors=\"coerce\")\n",
    "merged[\"objects_per_ml\"] = pd.to_numeric(merged[\"objects_per_ml\"], errors=\"coerce\")\n",
    "\n",
    "os.makedirs(os.path.dirname(out_file), exist_ok=True)\n",
    "merged.to_csv(out_file, index=False)\n",
    "\n",
    "print(f\"Wrote merged file: {out_file}\")\n",
    "print(merged.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
