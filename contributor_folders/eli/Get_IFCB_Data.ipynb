{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get IFCB Data\n",
    "\n",
    "This is from a notebook written by Gulce Kurtay during OceanHackWeek 2024. [notebook](https://github.com/oceanhackweek/ohw24_proj_pace_us/blob/main/final_notebooks/Hypercoast(PACE)_with_IFCB.ipynb)\n",
    "\n",
    "Woods Hole Dashboard, https://ifcb-data.whoi.edu/dashboard <br>\n",
    "\n",
    "This notebook designed to get in-situ flow cytometry group distribution data.<br>\n",
    "Classified IFCB data is limited, so make sure to search the database then find a group distribution that matches with your interest. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1  download the phytoplankton group distribution and metadata from the IFCB dashboard, it only downloads the autoclass files not the images<br>\n",
    "STEP 2.1 organize the csv files in group distribution format aligns with lat and long info<br>\n",
    "STEP 2.2 summarize the information into daily format to match up with PACE data <br>\n",
    "STEP 3   Download the PACE data with hypercoast<br>\n",
    "STEP 4   interactive map with spatial chl-a and insitu group distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1.1 \n",
    "#Functions that needed for the downloading the csv files\n",
    "#STEP-1 Download the csv files and lat and long\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def collect_bin_ids(start_bin_id, end_bin_id, base_url, dataset, instrument, prefix):\n",
    "    \"\"\"\n",
    "    Collects all bin IDs starting with a given prefix and stops at the end bin ID.\n",
    "\n",
    "    :param start_bin_id: The starting bin ID to begin the search.\n",
    "    :param end_bin_id: The bin ID at which to stop collecting.\n",
    "    :param base_url: The base URL of the API.\n",
    "    :param dataset: The dataset name to filter.\n",
    "    :param instrument: The instrument name to filter.\n",
    "    :param prefix: The prefix to match bin IDs against (e.g., \"D2024\").\n",
    "    :return: A list of matching bin IDs.\n",
    "    \"\"\"\n",
    "    bin_ids = []\n",
    "    current_bin_id = start_bin_id\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{base_url}/api/bin/{current_bin_id}\"\n",
    "        params = {\n",
    "            \"dataset\": dataset,\n",
    "            \"instrument\": instrument,\n",
    "        }\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data for bin: {current_bin_id}, Status Code: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        if current_bin_id.startswith(prefix):\n",
    "            bin_ids.append(current_bin_id)\n",
    "            print(f\"Collected bin ID: {current_bin_id}\")\n",
    "        \n",
    "        # Check if we've reached the end bin ID\n",
    "        if current_bin_id == end_bin_id:\n",
    "            print(f\"Reached the end bin ID: {end_bin_id}\")\n",
    "            break\n",
    "        \n",
    "        next_bin_id = data.get('next_bin_id')\n",
    "        if not next_bin_id or not next_bin_id.startswith(prefix):\n",
    "            break\n",
    "        \n",
    "        current_bin_id = next_bin_id\n",
    "    \n",
    "    print(f\"Total bin IDs collected: {len(bin_ids)}\")\n",
    "    return bin_ids\n",
    "\n",
    "def download_file(file_url, output_dir):\n",
    "    \"\"\"\n",
    "    Downloads a specific file from the given URL.\n",
    "\n",
    "    :param file_url: The full URL to the file.\n",
    "    :param output_dir: The directory where the file will be saved.\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_url)\n",
    "    output_file = os.path.join(output_dir, file_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(file_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(output_file, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded: {output_file}\")\n",
    "        return True\n",
    "\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        if response.status_code == 404:\n",
    "            print(f\"File not found (404): {file_url}\")\n",
    "        else:\n",
    "            print(f\"HTTP error occurred: {err} - URL: {file_url}\")\n",
    "        return False\n",
    "    except requests.exceptions.ConnectionError as err:\n",
    "        print(f\"Connection error occurred: {err}\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout as err:\n",
    "        print(f\"Timeout error occurred: {err}\")\n",
    "        return False\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"An error occurred: {err}\")\n",
    "        return False\n",
    "\n",
    "def fetch_lat_lon(bin_id, base_url, dataset):\n",
    "    \"\"\"\n",
    "    Fetches latitude and longitude for a specific bin ID.\n",
    "\n",
    "    :param bin_id: The bin ID to fetch lat/lon for.\n",
    "    :param base_url: The base URL where the dataset is located.\n",
    "    :param dataset: The dataset name.\n",
    "    :return: A dictionary with latitude and longitude.\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/api/bin/{bin_id}\"\n",
    "    params = {\n",
    "        \"dataset\": dataset,\n",
    "    }\n",
    "    response = requests.get(url, params=params, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return {\n",
    "            \"bin_id\": bin_id,\n",
    "            \"latitude\": data.get('lat'),\n",
    "            \"longitude\": data.get('lng')\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch lat/lon for bin: {bin_id}, Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def download_autoclass_csvs_and_lat_lon(start_bin_id, end_bin_id, base_url, dataset, instrument, prefix, output_dir, max_workers=5):\n",
    "    \"\"\"\n",
    "    Collects bin IDs, downloads all _class_scores.csv files, and fetches latitude and longitude for each bin.\n",
    "\n",
    "    :param start_bin_id: The starting bin ID to begin the search.\n",
    "    :param end_bin_id: The bin ID at which to stop collecting.\n",
    "    :param base_url: The base URL where the dataset is located.\n",
    "    :param dataset: The dataset name to filter.\n",
    "    :param instrument: The instrument name to filter.\n",
    "    :param prefix: The prefix to match bin IDs against (e.g., \"D2024\").\n",
    "    :param output_dir: The directory where the files will be saved.\n",
    "    :param max_workers: The maximum number of parallel downloads.\n",
    "    \"\"\"\n",
    "    bin_ids = collect_bin_ids(start_bin_id, end_bin_id, base_url, dataset, instrument, prefix)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        lat_lon_results = []\n",
    "        \n",
    "        for bin_id in bin_ids:\n",
    "            # Download the _class_scores.csv file\n",
    "            csv_file_url = f\"{base_url}/{dataset}/{bin_id}_class_scores.csv\"\n",
    "            futures.append(executor.submit(download_file, csv_file_url, output_dir))\n",
    "            \n",
    "            # Fetch latitude and longitude\n",
    "            lat_lon_results.append(fetch_lat_lon(bin_id, base_url, dataset))\n",
    "        \n",
    "        # Wait for all downloads to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "        \n",
    "        # Extract date from start_bin_id for the CSV filename\n",
    "        date_str = start_bin_id.split('T')[0][1:]  # Extracts \"DYYYYMMDD\" and removes the \"D\"\n",
    "        lat_lon_file = os.path.join(output_dir, f\"{date_str}.csv\")\n",
    "        \n",
    "        # Write lat/lon to CSV\n",
    "        if lat_lon_results:\n",
    "            with open(lat_lon_file, 'w', newline='') as csvfile:\n",
    "                fieldnames = ['bin_id', 'latitude', 'longitude']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                \n",
    "                writer.writeheader()\n",
    "                for result in lat_lon_results:\n",
    "                    if result:\n",
    "                        writer.writerow(result)\n",
    "            print(f\"Latitude and Longitude data saved to {lat_lon_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1.2 application\n",
    "#To get the community composition information from the dashboard, you will need to know specific information about the dataset:\n",
    "#I have added stat and end bin ids because it will take too much time, so you can limit it \n",
    "# Example usage\n",
    "base_url = \"https://ifcb-data.whoi.edu\"\n",
    "dataset = \"mvco\" #Dataset name: Name should be as it spelled in the url \n",
    "instrument = \"IFCB10\" #Instrument number\n",
    "# Roll over the timeline and then you can copy the bin number\n",
    "start_bin_id = \"D20251101T232410_IFCB127\"  # Start with a known valid bin ID \n",
    "end_bin_id = \"D20251102T000857_IFCB127\"  # End at this bin ID\n",
    "#end_bin_id = \"D20241227T181716_IFCB010\"  # End at this bin ID\n",
    "prefix = \"D2025\"  # Prefix to match bin IDs\n",
    "output_dir = r\"../../data/mvco\"  # Directory to save the files\n",
    "\n",
    "# Download all _class_scores.csv files and fetch lat/lon data, stopping at end_bin_id\n",
    "download_autoclass_csvs_and_lat_lon(start_bin_id, end_bin_id, base_url, dataset, instrument, prefix, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2.1, SPATIAL GROUPING FOR ONE DAY\n",
    "#Choose the group with highest score for eVery images\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "if_files_root = r\"../../data/mvco\" # this is the folder pathway to your local computer \n",
    "\n",
    "# List to store summary data for each file\n",
    "all_summaries = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for file_name in os.listdir(if_files_root):\n",
    "    if file_name.startswith('D2024') and file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(if_files_root, file_name)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Keep the 'pid' column and process the numeric columns for max values\n",
    "        pid_column = df['pid']\n",
    "        numeric_df = df.iloc[:, 1:].fillna(-float('inf'))  # Process the numeric columns, excluding 'pid'\n",
    "        \n",
    "        # Find the max value and corresponding column\n",
    "        df['max_value_column'] = numeric_df.idxmax(axis=1)\n",
    "        df['max_value'] = numeric_df.max(axis=1)\n",
    "        \n",
    "        # Create a summary DataFrame, including 'pid', 'max_value_column', and 'max_value'\n",
    "        summary_df = pd.DataFrame({\n",
    "            'pid': pid_column,\n",
    "            'max_value_column': df['max_value_column'],\n",
    "            'max_value': df['max_value']\n",
    "        })\n",
    "        \n",
    "        # Extract the date from the file name and remove the leading \"D\"\n",
    "        summary_df['date'] = file_name.split('T')[0][1:]\n",
    "        \n",
    "        # Append the summary to the list\n",
    "        all_summaries.append(summary_df)\n",
    "\n",
    "# Concatenate all summary DataFrames into one\n",
    "final_summary = pd.concat(all_summaries, ignore_index=True)\n",
    "\n",
    "# Print the final summary (or save it to a file if needed)\n",
    "print(final_summary)\n",
    "\n",
    "# Optionally, save the final summary to a CSV file\n",
    "final_summary.to_csv('../../data/final_summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>max_value_column</th>\n",
       "      <th>max_value</th>\n",
       "      <th>date</th>\n",
       "      <th>pid_date</th>\n",
       "      <th>bin_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>bin_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D20240215T150055_IFCB010_00001</td>\n",
       "      <td>nanoplankton_mix</td>\n",
       "      <td>0.7495</td>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>D20240215T150055</td>\n",
       "      <td>D20240215T150055_IFCB010</td>\n",
       "      <td>41.325</td>\n",
       "      <td>-70.5667</td>\n",
       "      <td>D20240215T150055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D20240215T150055_IFCB010_00002</td>\n",
       "      <td>fiber</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>D20240215T150055</td>\n",
       "      <td>D20240215T150055_IFCB010</td>\n",
       "      <td>41.325</td>\n",
       "      <td>-70.5667</td>\n",
       "      <td>D20240215T150055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D20240215T150055_IFCB010_00003</td>\n",
       "      <td>Rhizosolenia</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>D20240215T150055</td>\n",
       "      <td>D20240215T150055_IFCB010</td>\n",
       "      <td>41.325</td>\n",
       "      <td>-70.5667</td>\n",
       "      <td>D20240215T150055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D20240215T150055_IFCB010_00004</td>\n",
       "      <td>detritus</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>D20240215T150055</td>\n",
       "      <td>D20240215T150055_IFCB010</td>\n",
       "      <td>41.325</td>\n",
       "      <td>-70.5667</td>\n",
       "      <td>D20240215T150055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D20240215T150055_IFCB010_00005</td>\n",
       "      <td>detritus</td>\n",
       "      <td>0.9620</td>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>D20240215T150055</td>\n",
       "      <td>D20240215T150055_IFCB010</td>\n",
       "      <td>41.325</td>\n",
       "      <td>-70.5667</td>\n",
       "      <td>D20240215T150055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pid  max_value_column  max_value       date  \\\n",
       "0  D20240215T150055_IFCB010_00001  nanoplankton_mix     0.7495 2024-02-15   \n",
       "1  D20240215T150055_IFCB010_00002             fiber     0.9995 2024-02-15   \n",
       "2  D20240215T150055_IFCB010_00003      Rhizosolenia     0.9980 2024-02-15   \n",
       "3  D20240215T150055_IFCB010_00004          detritus     1.0000 2024-02-15   \n",
       "4  D20240215T150055_IFCB010_00005          detritus     0.9620 2024-02-15   \n",
       "\n",
       "           pid_date                    bin_id  latitude  longitude  \\\n",
       "0  D20240215T150055  D20240215T150055_IFCB010    41.325   -70.5667   \n",
       "1  D20240215T150055  D20240215T150055_IFCB010    41.325   -70.5667   \n",
       "2  D20240215T150055  D20240215T150055_IFCB010    41.325   -70.5667   \n",
       "3  D20240215T150055  D20240215T150055_IFCB010    41.325   -70.5667   \n",
       "4  D20240215T150055  D20240215T150055_IFCB010    41.325   -70.5667   \n",
       "\n",
       "           bin_date  \n",
       "0  D20240215T150055  \n",
       "1  D20240215T150055  \n",
       "2  D20240215T150055  \n",
       "3  D20240215T150055  \n",
       "4  D20240215T150055  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now load the data\n",
    "difcb=pd.read_csv(\"../../data/mvco_2024.csv\")\n",
    "# Convert the cleaned date strings to datetime objects\n",
    "difcb['date'] = pd.to_datetime(difcb['date'], format='%Y%m%d')\n",
    "difcb.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now go to Hypercoast notebook\n",
    "\n",
    "To see how they got some PACE with hypercoast package--if you want."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
